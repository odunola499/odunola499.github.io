---
title: "The Problem with Open Source LLMs for Small and Medium Enterprises (SMEs) Adoption"
date: 2023-9-27
categories: [AI Use]
tags: [runpod, sentence-transformers, llama, falcon]
---
![A cute llama](artifacts/sme/1_P6CfkG6EXYbbrL-2OrFpkw.webp)

A few weeks ago, I started a project to build a basic search engine from scratch using a proprietary dataset. The architecture was straightforward. First, I split the dataset into smaller, manageable parts. Then, I used an encoder-only language model to generate vector embeddings for the text. These embeddings, along with their corresponding data segments, were stored in a vector database. 

Initially, I considered using cloud services like Pinecone or Weaviate’s Weaviate Cloud Service to streamline retrieval. However, I decided to challenge myself by building the entire application with only open-source frameworks and models, avoiding options like OpenAI and Pinecone. Sounds exciting, right? This isn’t a tutorial, just me sharing the process.

Open-source large language models like Llama2 and Falcon have made a big impact in AI. They’re recognized for their strong performance and high-quality responses. What’s special about them is that they’re available under a commercial license, making them suitable for enterprises that need secure solutions. This is especially important for industries dealing with sensitive data, like healthcare, where privacy is a priority.

Llama2 70B and the newly released Falcon 180B are currently leading HuggingFace’s open LLM Leaderboard, which tracks performance. These models are trained on massive datasets, fine-tuned, and enhanced with Reinforcement Learning through human feedback. This approach helps align their responses better with human expectations.

[Fine-tuned versions of Llama2 70B have been rumoured to compete with GPT-3.5 Turbo for specific tasks](https://x.com/TheTuringPost/status/1699767503399034957). There’s even talk that Falcon’s 160B model outperforms GPT-3.5 Turbo and approaches GPT-4’s level of performance. However, such claims should be treated carefully, as highlighted in a [video by James Briggs on YouTube.](https://www.youtube.com/watch?v=l3oNsGtRjqo)

![Finetuned Variants of LLama70B dominate the leaderboard](artifacts/sme/1_MyuBPscXqt4BKqtGvPsAFQ.webp)

Let’s get back to the project. We’re hitting an important step now. Generating embeddings for my data was pretty straightforward. I used the `intfloat/e5-large-v2` model from Hugging Face. This model ranks high on Hugging Face’s Text Embedding Leaderboard and, from my own experience, even outperforms OpenAI’s text embedding model in open-source tests.

To make implementation easier, I used the Sentence-Transformers library. One thing to note: using the Hugging Face model directly with Sentence-Transformers can cause errors because it lacks a specific tag needed by the library. To fix this, I cloned the model’s repository to my account, edited the ReadMe file, and added the missing tag.

On the hardware side, deep learning models need a GPU. Luckily, RunPod offered affordable GPU instances. I launched an instance with an L4 GPU and used it to generate my embeddings. For more details, you can check out Pinecone’s tutorial on vector embeddings.

Once the embeddings were ready, I uploaded both the text data and embeddings to a Weaviate Cloud Service instance. With that, the second goal was done.

Next, I started testing Llama7B on a RunPod instance with an L40 GPU and 48GB of VRAM. The model performed well on basic text generation tasks, with no fancy setup—just 8-bit quantization. Most importantly, it fit into the GPU’s memory.

![Result from LLama7b without context](artifacts/sme/1_rn61YKTLBCnlWQgUw1SOMA.webp)

However, after rewriting the code generation and giving it a context with the following prompt

```python
prompt = """
### Instruction:
[INST] <<SYS>> YOu are given a request "Request" and a context "Context" that would help you respond to the request. You are a world renowned chef that is well versed in all forms of cuisine and food related content. you also understand how to create dishes that avoid peoples allergies like gluten and nuts. Answer the person's request as best as you can given the context as a mode of referenceto you. If the person's request isnt food related please reply with "Please keep your request food-related" only. If the context "Context" does not seem to help you answer the requestproperly you can do without the Context and answer the question with your memory alone<</SYS>>
            Request: {request}
            Context: {context}
### Response:  [/INST]
            """
```

The output was a jumble of incoherent words, and it consumed a significant amount of time. My brief test was concluded, revealing that LLama2 7B, while relatively straightforward to implement and suitable for basic tasks, falls short when it comes to tasks demanding robust reasoning capabilities. Additionally, the execution time was considerably lengthy.

![execution](artifacts/sme/1_o8BZ0BGJHHbKyQ43GV9rNg.webp)

The output was a mess of random words and took a long time to generate. My quick test showed that Llama2 7B, while easy to use for basic tasks, struggles with more complex reasoning. Plus, the execution time was quite long.

To speed things up and reach a conclusion faster, I decided to try the Llama 70B model. This is where things got interesting. Setting it up was tough, and I lost count of the out-of-memory errors that kept popping up.

The model needed 160GB of VRAM to run in full precision, which is like having two A100 GPUs. Inference times were also slow, but it performed well and achieved what I wanted. However, it came with a huge cost in terms of resources and time.

There are clear benefits to owning and controlling your models, but getting peak performance during inference requires a big investment of time, skill, and money. Building an efficient system to serve models while keeping latency low is tough, especially for small businesses.

I know ways to cut down latency and reduce resource usage, like using 8 or 4-bit precision and using libraries like llama.cpp for faster inference. But lowering precision can reduce output quality. If you’re okay with a slight drop in performance, these options can work, but you should test thoroughly to see if they meet your needs.

Let’s talk about cost. When using cloud providers, you pay for the time your instance is active, even if your model isn’t doing anything for part of that time. This can be a problem for small businesses with fewer users, as they still get charged for the full active time. This is a lot more expensive compared to how closed-model companies charge based on tokens sent and generated.

Think of a healthcare startup that has sensitive data and wants to use AI, but doesn't want to send it through third-party APIs due to security risks. This is a tricky situation for them.

In the debate between open and closed-source models, I see open-source models as useful for speeding up AI research. But we need to keep in mind that we’re building these models not just for researchers, but for real-world products people can use.

I respect projects like BentoML for making AI model deployment easier. The open-source community should focus on making private models accessible and affordable for businesses, especially those handling sensitive data like health records, financial info, and trade secrets.

I believe the open-source community should not only focus on scaling models but also on creating better ways to deploy them. Finding efficient ways to serve models can be a key factor in winning the open vs. closed-source debate.

Back to the project, things went mostly as planned, but there were still challenges running the model. Thanks to RunPod, I got performance close to GPT-3.5. I had considered trying Falcon 180B, but it requires 400GB of VRAM, so I’m not so sure now.

Thanks for sticking around and reading my thoughts. Feel free to reach out to me on Twitter if you want to chat more or collaborate.
